{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is7Y5jF62NbS",
        "outputId": "2708ab57-b98a-44e1-f5fa-d869e1d6176d"
      },
      "outputs": [],
      "source": [
        "#!yes | pip uninstall torchvison\n",
        "#!pip install -qU torchvision\n",
        "#!pip install sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip2pCKBk2Nbb"
      },
      "source": [
        "# MNIST Training using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBncu3h-2Nbf"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. [Background](#Background)\n",
        "1. [Setup](#Setup)\n",
        "1. [Data](#Data)\n",
        "1. [Train](#Train)\n",
        "1. [Host](#Host)\n",
        "\n",
        "---\n",
        "\n",
        "## Background\n",
        "\n",
        "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
        "\n",
        "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
        "\n",
        "Let's start by creating a SageMaker session and specifying:\n",
        "\n",
        "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
        "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4ZTYgZJC2Nbj"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import os\n",
        "\n",
        "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIATGR2MRZAFA7WTQXF\"\n",
        "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAR3MFUGLUM4XEJ2KZ\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = \"data\"\n",
        "\n",
        "# role = \"arn:aws:iam::220239531584:role/service-role/AmazonSageMaker-ExecutionRole-20221130T191674\"\n",
        "role = \"arn:aws:iam::127518651112:role/service-role/AmazonSageMaker-ExecutionRole-20221201T173507\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ya1dYW2Nbk"
      },
      "source": [
        "## Data\n",
        "### Getting the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6OjazRY2Nbl",
        "outputId": "86c1c964-0c12-4f7f-a5cd-3d58f33aa827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/erg1/Desktop/SMDataScienceCourseContent/CS243-Computer-Networks/pipe-squeeze/src/models'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpP6733-2Nbn"
      },
      "source": [
        "### Uploading the data to S3\n",
        "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpgqsJ7i2Nbp",
        "outputId": "fdc7731e-86d8-4eb5-9da9-b569597b342d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-220239531584/sagemaker/pipe-squeeze\n"
          ]
        }
      ],
      "source": [
        "# inputs = sagemaker_session.upload_data(path=\"../../data/ImageNet\", bucket=bucket, key_prefix=prefix)\n",
        "# print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh9xhj192Nbr"
      },
      "source": [
        "## Train\n",
        "### Training script\n",
        "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
        "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
        "\n",
        "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
        "  These artifacts are uploaded to S3 for model hosting.\n",
        "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
        "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
        "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
        "\n",
        "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
        "\n",
        "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
        "\n",
        "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
        "\n",
        "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
        "\n",
        "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
        "\n",
        "For example, the script run by this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUg2HzJX2Nbt",
        "outputId": "452985a8-8dd5-4f83-eae7-1623c103d8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
            "\n",
            "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
            "\n",
            "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
            "logger.setLevel(logging.DEBUG)\n",
            "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
            "\n",
            "\n",
            "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\n",
            "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
            "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
            "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
            "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
            "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\n",
            "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\n",
            "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
            "\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
            "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\n",
            "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\n",
            "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\n",
            "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\n",
            "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\n",
            "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
            "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed, **kwargs):\n",
            "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    dataset = datasets.MNIST(\n",
            "        training_dir,\n",
            "        train=\u001b[34mTrue\u001b[39;49;00m,\n",
            "        transform=transforms.Compose(\n",
            "            [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\n",
            "        ),\n",
            "    )\n",
            "    train_sampler = (\n",
            "        torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
            "    )\n",
            "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
            "        dataset,\n",
            "        batch_size=batch_size,\n",
            "        shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\n",
            "        sampler=train_sampler,\n",
            "        **kwargs\n",
            "    )\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir, **kwargs):\n",
            "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
            "        datasets.MNIST(\n",
            "            training_dir,\n",
            "            train=\u001b[34mFalse\u001b[39;49;00m,\n",
            "            transform=transforms.Compose(\n",
            "                [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\n",
            "            ),\n",
            "        ),\n",
            "        batch_size=test_batch_size,\n",
            "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
            "        **kwargs\n",
            "    )\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\n",
            "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\n",
            "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\n",
            "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
            "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
            "        param.grad.data /= size\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
            "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
            "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\n",
            "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
            "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\n",
            "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "\n",
            "    \u001b[34mif\u001b[39;49;00m is_distributed:\n",
            "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\n",
            "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\n",
            "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\n",
            "        host_rank = args.hosts.index(args.current_host)\n",
            "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\n",
            "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
            "        logger.info(\n",
            "            \u001b[33m\"\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
            "                args.backend, dist.get_world_size()\n",
            "            )\n",
            "            + \u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(dist.get_rank(), args.num_gpus)\n",
            "        )\n",
            "\n",
            "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
            "    torch.manual_seed(args.seed)\n",
            "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
            "        torch.cuda.manual_seed(args.seed)\n",
            "\n",
            "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
            "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n",
            "\n",
            "    logger.debug(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
            "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
            "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
            "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
            "        )\n",
            "    )\n",
            "\n",
            "    logger.debug(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
            "            \u001b[36mlen\u001b[39;49;00m(test_loader.sampler),\n",
            "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
            "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
            "        )\n",
            "    )\n",
            "\n",
            "    model = Net().to(device)\n",
            "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\n",
            "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\n",
            "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
            "    \u001b[34melse\u001b[39;49;00m:\n",
            "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\n",
            "        model = torch.nn.DataParallel(model)\n",
            "\n",
            "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
            "\n",
            "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
            "        model.train()\n",
            "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
            "            data, target = data.to(device), target.to(device)\n",
            "            optimizer.zero_grad()\n",
            "            output = model(data)\n",
            "            loss = F.nll_loss(output, target)\n",
            "            loss.backward()\n",
            "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\n",
            "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\n",
            "                _average_gradients(model)\n",
            "            optimizer.step()\n",
            "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
            "                logger.info(\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
            "                        epoch,\n",
            "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(data),\n",
            "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
            "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\n",
            "                        loss.item(),\n",
            "                    )\n",
            "                )\n",
            "        test(model, test_loader, device)\n",
            "    save_model(model, args.model_dir)\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\n",
            "    model.eval()\n",
            "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
            "    correct = \u001b[34m0\u001b[39;49;00m\n",
            "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
            "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
            "            data, target = data.to(device), target.to(device)\n",
            "            output = model(data)\n",
            "            test_loss += F.nll_loss(output, target, size_average=\u001b[34mFalse\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
            "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
            "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
            "\n",
            "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
            "    logger.info(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
            "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
            "        )\n",
            "    )\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    model = torch.nn.DataParallel(Net())\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
            "        model.load_state_dict(torch.load(f))\n",
            "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
            "\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
            "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
            "    torch.save(model.cpu().state_dict(), path)\n",
            "\n",
            "\n",
            "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
            "    parser = argparse.ArgumentParser()\n",
            "\n",
            "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
            "        default=\u001b[34m64\u001b[39;49;00m,\n",
            "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
            "        default=\u001b[34m1000\u001b[39;49;00m,\n",
            "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
            "        default=\u001b[34m10\u001b[39;49;00m,\n",
            "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
            "    )\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
            "        default=\u001b[34m100\u001b[39;49;00m,\n",
            "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
            "        default=\u001b[34mNone\u001b[39;49;00m,\n",
            "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
            "    )\n",
            "\n",
            "    \u001b[37m# Container environment\u001b[39;49;00m\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
            "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
            "\n",
            "    train(parser.parse_args())\n"
          ]
        }
      ],
      "source": [
        "!pygmentize mnist.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX7TiWSe2Nbu"
      },
      "source": [
        "### Run training in SageMaker\n",
        "\n",
        "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Tafh7jiZ2Nbv"
      },
      "outputs": [],
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "n_pipelines = 2\n",
        "\n",
        "pt_estimator = PyTorch(\n",
        "    entry_point=\"train_sagemaker.py\",\n",
        "    source_dir=\"models\",\n",
        "    role=role,\n",
        "    instance_count=n_pipelines,\n",
        "    instance_type=\"ml.g4dn.12xlarge\",\n",
        "    framework_version='1.12.1',\n",
        "    py_version='py38',\n",
        "    hyperparameters={\"epochs\": 3, \"backend\": \"gloo\", \"batch-size\": 64, \"n-microbatches\": 4, \"learning-rate\": 0.003},\n",
        "    # image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
        "    # distribution={\n",
        "    #     \"pytorchddp\": {\n",
        "    #         \"enabled\": True\n",
        "    #     }\n",
        "    # },\n",
        "    base_job_name=\"pipe-squeeze-test\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnWmjvEt2Nbv"
      },
      "source": [
        "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFtlmgPr2Nbw",
        "outputId": "93a22368-134a-46ff-9d6c-845e9b80286a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-12-05 21:34:55 Starting - Starting the training job...\n",
            "2022-12-05 21:35:21 Starting - Preparing the instances for trainingProfilerReport-1670276095: InProgress\n",
            ".........\n",
            "2022-12-05 21:36:59 Downloading - Downloading input data...\n",
            "2022-12-05 21:37:26 Training - Downloading the training image........................\n",
            "2022-12-05 21:41:28 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
            "\u001b[34mbash: no job control in this shell\u001b[0m\n",
            "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
            "\u001b[35mbash: no job control in this shell\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:30,860 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:30,898 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:30,907 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:30,913 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:31,501 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:31,550 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:31,598 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[35m2022-12-05 21:41:31,609 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
            "\u001b[35mTraining Env:\u001b[0m\n",
            "\u001b[35m{\n",
            "    \"additional_framework_parameters\": {},\n",
            "    \"channel_input_dirs\": {\n",
            "        \"training\": \"/opt/ml/input/data/training\"\n",
            "    },\n",
            "    \"current_host\": \"algo-2\",\n",
            "    \"current_instance_group\": \"homogeneousCluster\",\n",
            "    \"current_instance_group_hosts\": [\n",
            "        \"algo-2\",\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
            "    \"distribution_hosts\": [],\n",
            "    \"distribution_instance_groups\": [],\n",
            "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
            "    \"hosts\": [\n",
            "        \"algo-1\",\n",
            "        \"algo-2\"\n",
            "    ],\n",
            "    \"hyperparameters\": {\n",
            "        \"backend\": \"gloo\",\n",
            "        \"batch-size\": 64,\n",
            "        \"epochs\": 3,\n",
            "        \"learning-rate\": 0.003,\n",
            "        \"n-microbatches\": 4\n",
            "    },\n",
            "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
            "    \"input_data_config\": {\n",
            "        \"training\": {\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        }\n",
            "    },\n",
            "    \"input_dir\": \"/opt/ml/input\",\n",
            "    \"instance_groups\": [\n",
            "        \"homogeneousCluster\"\n",
            "    ],\n",
            "    \"instance_groups_dict\": {\n",
            "        \"homogeneousCluster\": {\n",
            "            \"instance_group_name\": \"homogeneousCluster\",\n",
            "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
            "            \"hosts\": [\n",
            "                \"algo-2\",\n",
            "                \"algo-1\"\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    \"is_hetero\": false,\n",
            "    \"is_master\": false,\n",
            "    \"is_modelparallel_enabled\": null,\n",
            "    \"is_smddpmprun_installed\": false,\n",
            "    \"job_name\": \"pipe-squeeze-test-2022-12-05-21-34-54-461\",\n",
            "    \"log_level\": 20,\n",
            "    \"master_hostname\": \"algo-1\",\n",
            "    \"model_dir\": \"/opt/ml/model\",\n",
            "    \"module_dir\": \"s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\",\n",
            "    \"module_name\": \"train_sagemaker\",\n",
            "    \"network_interface_name\": \"eth0\",\n",
            "    \"num_cpus\": 48,\n",
            "    \"num_gpus\": 4,\n",
            "    \"num_neurons\": 0,\n",
            "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
            "    \"output_dir\": \"/opt/ml/output\",\n",
            "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
            "    \"resource_config\": {\n",
            "        \"current_host\": \"algo-2\",\n",
            "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
            "        \"current_group_name\": \"homogeneousCluster\",\n",
            "        \"hosts\": [\n",
            "            \"algo-1\",\n",
            "            \"algo-2\"\n",
            "        ],\n",
            "        \"instance_groups\": [\n",
            "            {\n",
            "                \"instance_group_name\": \"homogeneousCluster\",\n",
            "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
            "                \"hosts\": [\n",
            "                    \"algo-2\",\n",
            "                    \"algo-1\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"network_interface_name\": \"eth0\"\n",
            "    },\n",
            "    \"user_entry_point\": \"train_sagemaker.py\"\u001b[0m\n",
            "\u001b[35m}\u001b[0m\n",
            "\u001b[35mEnvironment variables:\u001b[0m\n",
            "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
            "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
            "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":3,\"learning-rate\":0.003,\"n-microbatches\":4}\u001b[0m\n",
            "\u001b[35mSM_USER_ENTRY_POINT=train_sagemaker.py\u001b[0m\n",
            "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
            "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
            "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
            "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
            "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
            "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
            "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
            "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
            "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
            "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
            "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
            "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
            "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
            "\u001b[35mSM_MODULE_NAME=train_sagemaker\u001b[0m\n",
            "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
            "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
            "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
            "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
            "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
            "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
            "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
            "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
            "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
            "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\u001b[0m\n",
            "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":3,\"learning-rate\":0.003,\"n-microbatches\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pipe-squeeze-test-2022-12-05-21-34-54-461\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\",\"module_name\":\"train_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_sagemaker.py\"}\u001b[0m\n",
            "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"3\",\"--learning-rate\",\"0.003\",\"--n-microbatches\",\"4\"]\u001b[0m\n",
            "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
            "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
            "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
            "\u001b[35mSM_HP_BATCH-SIZE=64\u001b[0m\n",
            "\u001b[35mSM_HP_EPOCHS=3\u001b[0m\n",
            "\u001b[35mSM_HP_LEARNING-RATE=0.003\u001b[0m\n",
            "\u001b[35mSM_HP_N-MICROBATCHES=4\u001b[0m\n",
            "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221118-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
            "\u001b[35mInvoking script with the following command:\u001b[0m\n",
            "\u001b[35m/opt/conda/bin/python3.8 train_sagemaker.py --backend gloo --batch-size 64 --epochs 3 --learning-rate 0.003 --n-microbatches 4\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:31,353 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:31,390 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:31,399 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:31,409 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:32,191 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:32,239 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:32,288 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
            "\u001b[34m2022-12-05 21:41:32,300 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
            "\u001b[34mTraining Env:\u001b[0m\n",
            "\u001b[34m{\n",
            "    \"additional_framework_parameters\": {},\n",
            "    \"channel_input_dirs\": {\n",
            "        \"training\": \"/opt/ml/input/data/training\"\n",
            "    },\n",
            "    \"current_host\": \"algo-1\",\n",
            "    \"current_instance_group\": \"homogeneousCluster\",\n",
            "    \"current_instance_group_hosts\": [\n",
            "        \"algo-2\",\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
            "    \"distribution_hosts\": [],\n",
            "    \"distribution_instance_groups\": [],\n",
            "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
            "    \"hosts\": [\n",
            "        \"algo-1\",\n",
            "        \"algo-2\"\n",
            "    ],\n",
            "    \"hyperparameters\": {\n",
            "        \"backend\": \"gloo\",\n",
            "        \"batch-size\": 64,\n",
            "        \"epochs\": 3,\n",
            "        \"learning-rate\": 0.003,\n",
            "        \"n-microbatches\": 4\n",
            "    },\n",
            "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
            "    \"input_data_config\": {\n",
            "        \"training\": {\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        }\n",
            "    },\n",
            "    \"input_dir\": \"/opt/ml/input\",\n",
            "    \"instance_groups\": [\n",
            "        \"homogeneousCluster\"\n",
            "    ],\n",
            "    \"instance_groups_dict\": {\n",
            "        \"homogeneousCluster\": {\n",
            "            \"instance_group_name\": \"homogeneousCluster\",\n",
            "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
            "            \"hosts\": [\n",
            "                \"algo-2\",\n",
            "                \"algo-1\"\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    \"is_hetero\": false,\n",
            "    \"is_master\": true,\n",
            "    \"is_modelparallel_enabled\": null,\n",
            "    \"is_smddpmprun_installed\": false,\n",
            "    \"job_name\": \"pipe-squeeze-test-2022-12-05-21-34-54-461\",\n",
            "    \"log_level\": 20,\n",
            "    \"master_hostname\": \"algo-1\",\n",
            "    \"model_dir\": \"/opt/ml/model\",\n",
            "    \"module_dir\": \"s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\",\n",
            "    \"module_name\": \"train_sagemaker\",\n",
            "    \"network_interface_name\": \"eth0\",\n",
            "    \"num_cpus\": 48,\n",
            "    \"num_gpus\": 4,\n",
            "    \"num_neurons\": 0,\n",
            "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
            "    \"output_dir\": \"/opt/ml/output\",\n",
            "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
            "    \"resource_config\": {\n",
            "        \"current_host\": \"algo-1\",\n",
            "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
            "        \"current_group_name\": \"homogeneousCluster\",\n",
            "        \"hosts\": [\n",
            "            \"algo-1\",\n",
            "            \"algo-2\"\n",
            "        ],\n",
            "        \"instance_groups\": [\n",
            "            {\n",
            "                \"instance_group_name\": \"homogeneousCluster\",\n",
            "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
            "                \"hosts\": [\n",
            "                    \"algo-2\",\n",
            "                    \"algo-1\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"network_interface_name\": \"eth0\"\n",
            "    },\n",
            "    \"user_entry_point\": \"train_sagemaker.py\"\u001b[0m\n",
            "\u001b[34m}\u001b[0m\n",
            "\u001b[34mEnvironment variables:\u001b[0m\n",
            "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
            "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
            "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":3,\"learning-rate\":0.003,\"n-microbatches\":4}\u001b[0m\n",
            "\u001b[34mSM_USER_ENTRY_POINT=train_sagemaker.py\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
            "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
            "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
            "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
            "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
            "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
            "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
            "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
            "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
            "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
            "\u001b[34mSM_MODULE_NAME=train_sagemaker\u001b[0m\n",
            "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
            "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
            "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
            "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
            "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
            "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
            "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
            "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\u001b[0m\n",
            "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":64,\"epochs\":3,\"learning-rate\":0.003,\"n-microbatches\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pipe-squeeze-test-2022-12-05-21-34-54-461\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-127518651112/pipe-squeeze-test-2022-12-05-21-34-54-461/source/sourcedir.tar.gz\",\"module_name\":\"train_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_sagemaker.py\"}\u001b[0m\n",
            "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"64\",\"--epochs\",\"3\",\"--learning-rate\",\"0.003\",\"--n-microbatches\",\"4\"]\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
            "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
            "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
            "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
            "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
            "\u001b[34mSM_HP_LEARNING-RATE=0.003\u001b[0m\n",
            "\u001b[34mSM_HP_N-MICROBATCHES=4\u001b[0m\n",
            "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221118-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
            "\u001b[34mInvoking script with the following command:\u001b[0m\n",
            "\u001b[34m/opt/conda/bin/python3.8 train_sagemaker.py --backend gloo --batch-size 64 --epochs 3 --learning-rate 0.003 --n-microbatches 4\u001b[0m\n",
            "\u001b[35mDistributed training - True\u001b[0m\n",
            "\u001b[35mNumber of gpus available - 4\u001b[0m\n",
            "\u001b[34mDistributed training - True\u001b[0m\n",
            "\u001b[34mNumber of gpus available - 4\u001b[0m\n",
            "\u001b[35mTotal parameters in model: 143,667,240\u001b[0m\n",
            "\u001b[35mTotal parameters in stage 0: 260,160\u001b[0m\n",
            "\u001b[35mTotal parameters in stage 1: 2,065,408\u001b[0m\n",
            "\u001b[35mTotal parameters in stage 2: 8,259,584\u001b[0m\n",
            "\u001b[35mTotal parameters in stage 3: 133,082,088\u001b[0m\n",
            "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 4 | cuda.device_count: 4\u001b[0m\n",
            "\u001b[35mCreate data loader rank 1\u001b[0m\n",
            "\u001b[35mWorld size: 2, setting effective batch size to 32. Should be batch size / num input gpus.\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.017 algo-2:92 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.197 algo-2:92 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.198 algo-2:92 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.198 algo-2:92 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.199 algo-2:92 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
            "\u001b[35m[2022-12-05 21:41:49.199 algo-2:92 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
            "\u001b[35mProcesses 4735/9469 (50%) of train data\u001b[0m\n",
            "\u001b[35mDEBUG:__main__:Processes 4735/9469 (50%) of train data\u001b[0m\n",
            "\u001b[35mProcesses 1963/3925 (50%) of test data\u001b[0m\n",
            "\u001b[35mDEBUG:__main__:Processes 1963/3925 (50%) of test data\u001b[0m\n",
            "\u001b[34mTotal parameters in model: 143,667,240\u001b[0m\n",
            "\u001b[34mTotal parameters in stage 0: 260,160\u001b[0m\n",
            "\u001b[34mTotal parameters in stage 1: 2,065,408\u001b[0m\n",
            "\u001b[34mTotal parameters in stage 2: 8,259,584\u001b[0m\n",
            "\u001b[34mTotal parameters in stage 3: 133,082,088\u001b[0m\n",
            "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 4 | cuda.device_count: 4\u001b[0m\n",
            "\u001b[34mCreate data loader rank 0\u001b[0m\n",
            "\u001b[34mWorld size: 2, setting effective batch size to 32. Should be batch size / num input gpus.\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.018 algo-1:91 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.206 algo-1:91 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.207 algo-1:91 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.207 algo-1:91 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.207 algo-1:91 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
            "\u001b[34m[2022-12-05 21:41:49.208 algo-1:91 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
            "\u001b[34mProcesses 4735/9469 (50%) of train data\u001b[0m\n",
            "\u001b[34mDEBUG:__main__:Processes 4735/9469 (50%) of train data\u001b[0m\n",
            "\u001b[34mDEBUG:__main__:Processes 1963/3925 (50%) of test data\u001b[0m\n",
            "\u001b[34mProcesses 1963/3925 (50%) of test data\u001b[0m\n",
            "\u001b[35mStart Training device 1\u001b[0m\n",
            "\u001b[35mINFO:__main__:Start Training device 1\u001b[0m\n",
            "\u001b[35m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[34mStart Training device 0\u001b[0m\n",
            "\u001b[34mINFO:__main__:Start Training device 0\u001b[0m\n",
            "\u001b[34m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[35m1%|          | 1/148 [00:10<25:27, 10.39s/it]\u001b[0m\n",
            "\u001b[35m[W logger.cpp:314] Warning: Cuda time stats are not collected for multi-device modules. (function operator())\u001b[0m\n",
            "\u001b[35mINFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
            "\u001b[34m1%|          | 1/148 [00:10<25:28, 10.40s/it]\u001b[0m\n",
            "\u001b[34m[W logger.cpp:314] Warning: Cuda time stats are not collected for multi-device modules. (function operator())\u001b[0m\n",
            "\u001b[34mINFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
            "\u001b[35m1%|▏         | 2/148 [00:11<12:23,  5.09s/it]\u001b[0m\n",
            "\u001b[34m1%|▏         | 2/148 [00:11<12:23,  5.09s/it]\u001b[0m\n",
            "\u001b[35m2%|▏         | 3/148 [00:12<08:01,  3.32s/it]\u001b[0m\n",
            "\u001b[34m2%|▏         | 3/148 [00:12<08:02,  3.32s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 4/148 [00:14<05:59,  2.49s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 4/148 [00:14<05:59,  2.49s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 5/148 [00:15<04:50,  2.03s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 5/148 [00:15<04:50,  2.03s/it]\u001b[0m\n",
            "\u001b[35m4%|▍         | 6/148 [00:16<04:09,  1.76s/it]\u001b[0m\n",
            "\u001b[34m4%|▍         | 6/148 [00:16<04:09,  1.76s/it]\u001b[0m\n",
            "\u001b[35m5%|▍         | 7/148 [00:17<03:43,  1.58s/it]\u001b[0m\n",
            "\u001b[34m5%|▍         | 7/148 [00:17<03:43,  1.58s/it]\u001b[0m\n",
            "\u001b[35m5%|▌         | 8/148 [00:19<03:25,  1.47s/it]\u001b[0m\n",
            "\u001b[34m5%|▌         | 8/148 [00:19<03:25,  1.47s/it]\u001b[0m\n",
            "\u001b[35m6%|▌         | 9/148 [00:20<03:13,  1.39s/it]\u001b[0m\n",
            "\u001b[34m6%|▌         | 9/148 [00:20<03:13,  1.39s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 10/148 [00:21<03:04,  1.34s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 10/148 [00:21<03:04,  1.34s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 11/148 [00:22<02:58,  1.30s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 11/148 [00:22<02:58,  1.30s/it]\u001b[0m\n",
            "\u001b[35m8%|▊         | 12/148 [00:23<02:53,  1.28s/it]\u001b[0m\n",
            "\u001b[34m8%|▊         | 12/148 [00:23<02:53,  1.28s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 13/148 [00:25<02:50,  1.26s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 13/148 [00:25<02:50,  1.26s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 14/148 [00:26<02:47,  1.25s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 14/148 [00:26<02:47,  1.25s/it]\u001b[0m\n",
            "\u001b[35m10%|█         | 15/148 [00:27<02:44,  1.24s/it]\u001b[0m\n",
            "\u001b[34m10%|█         | 15/148 [00:27<02:44,  1.24s/it]\u001b[0m\n",
            "\u001b[35m11%|█         | 16/148 [00:28<02:42,  1.23s/it]\u001b[0m\n",
            "\u001b[34m11%|█         | 16/148 [00:28<02:42,  1.23s/it]\u001b[0m\n",
            "\u001b[35m11%|█▏        | 17/148 [00:30<02:41,  1.23s/it]\u001b[0m\n",
            "\u001b[34m11%|█▏        | 17/148 [00:30<02:41,  1.23s/it]\u001b[0m\n",
            "\u001b[35m12%|█▏        | 18/148 [00:31<02:39,  1.23s/it]\u001b[0m\n",
            "\u001b[34m12%|█▏        | 18/148 [00:31<02:39,  1.23s/it]\u001b[0m\n",
            "\u001b[35m13%|█▎        | 19/148 [00:32<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m13%|█▎        | 19/148 [00:32<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▎        | 20/148 [00:33<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▎        | 20/148 [00:33<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▍        | 21/148 [00:34<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▍        | 21/148 [00:34<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m15%|█▍        | 22/148 [00:36<02:33,  1.22s/it]\u001b[0m\n",
            "\u001b[34m15%|█▍        | 22/148 [00:36<02:33,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 23/148 [00:37<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 23/148 [00:37<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 24/148 [00:38<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 24/148 [00:38<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m17%|█▋        | 25/148 [00:39<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m17%|█▋        | 25/148 [00:39<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 26/148 [00:41<02:28,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 26/148 [00:41<02:28,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 27/148 [00:42<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 27/148 [00:42<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m19%|█▉        | 28/148 [00:43<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m19%|█▉        | 28/148 [00:43<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|█▉        | 29/148 [00:44<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|█▉        | 29/148 [00:44<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|██        | 30/148 [00:45<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|██        | 30/148 [00:45<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m21%|██        | 31/148 [00:47<02:22,  1.22s/it]\u001b[0m\n",
            "\u001b[34m21%|██        | 31/148 [00:47<02:22,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 32/148 [00:48<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 32/148 [00:48<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 33/148 [00:49<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 33/148 [00:49<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m23%|██▎       | 34/148 [00:50<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m23%|██▎       | 34/148 [00:50<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▎       | 35/148 [00:52<02:17,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▎       | 35/148 [00:52<02:17,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▍       | 36/148 [00:53<02:16,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▍       | 36/148 [00:53<02:16,  1.22s/it]\u001b[0m\n",
            "\u001b[35m25%|██▌       | 37/148 [00:54<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m25%|██▌       | 37/148 [00:54<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▌       | 38/148 [00:55<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▌       | 38/148 [00:55<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▋       | 39/148 [00:56<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▋       | 39/148 [00:56<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m27%|██▋       | 40/148 [00:58<02:11,  1.22s/it]\u001b[0m\n",
            "\u001b[34m27%|██▋       | 40/148 [00:58<02:11,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 41/148 [00:59<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 41/148 [00:59<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 42/148 [01:00<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 42/148 [01:00<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m29%|██▉       | 43/148 [01:01<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m29%|██▉       | 43/148 [01:01<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|██▉       | 44/148 [01:03<02:06,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|██▉       | 44/148 [01:03<02:06,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|███       | 45/148 [01:04<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|███       | 45/148 [01:04<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m31%|███       | 46/148 [01:05<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m31%|███       | 46/148 [01:05<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 47/148 [01:06<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 47/148 [01:06<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 48/148 [01:07<02:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 48/148 [01:07<02:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m33%|███▎      | 49/148 [01:09<02:00,  1.22s/it]\u001b[0m\n",
            "\u001b[34m33%|███▎      | 49/148 [01:09<02:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 50/148 [01:10<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 50/148 [01:10<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 51/148 [01:11<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 51/148 [01:11<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m35%|███▌      | 52/148 [01:12<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m35%|███▌      | 52/148 [01:12<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▌      | 53/148 [01:13<01:55,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▌      | 53/148 [01:13<01:55,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▋      | 54/148 [01:15<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▋      | 54/148 [01:15<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m37%|███▋      | 55/148 [01:16<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m37%|███▋      | 55/148 [01:16<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m38%|███▊      | 56/148 [01:17<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m38%|███▊      | 56/148 [01:17<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▊      | 57/148 [01:18<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▊      | 57/148 [01:18<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▉      | 58/148 [01:20<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▉      | 58/148 [01:20<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[35m40%|███▉      | 59/148 [01:21<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m40%|███▉      | 59/148 [01:21<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 60/148 [01:22<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 60/148 [01:22<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 61/148 [01:23<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 61/148 [01:23<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m42%|████▏     | 62/148 [01:24<01:44,  1.22s/it]\u001b[0m\n",
            "\u001b[34m42%|████▏     | 62/148 [01:24<01:44,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 63/148 [01:26<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 63/148 [01:26<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 64/148 [01:27<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 64/148 [01:27<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m44%|████▍     | 65/148 [01:28<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m44%|████▍     | 65/148 [01:28<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▍     | 66/148 [01:29<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▍     | 66/148 [01:29<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▌     | 67/148 [01:31<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▌     | 67/148 [01:31<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[35m46%|████▌     | 68/148 [01:32<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m46%|████▌     | 68/148 [01:32<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 69/148 [01:33<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 69/148 [01:33<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 70/148 [01:34<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m48%|████▊     | 71/148 [01:35<01:33,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▊     | 72/148 [01:37<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▉     | 73/148 [01:38<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 70/148 [01:34<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m48%|████▊     | 71/148 [01:35<01:33,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▊     | 72/148 [01:37<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▉     | 73/148 [01:38<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m50%|█████     | 74/148 [01:39<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m50%|█████     | 74/148 [01:39<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████     | 75/148 [01:40<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████     | 75/148 [01:40<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████▏    | 76/148 [01:42<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████▏    | 76/148 [01:42<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m52%|█████▏    | 77/148 [01:43<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m52%|█████▏    | 77/148 [01:43<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 78/148 [01:44<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 78/148 [01:44<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 79/148 [01:45<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 79/148 [01:45<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m54%|█████▍    | 80/148 [01:46<01:22,  1.22s/it]\u001b[0m\n",
            "\u001b[34m54%|█████▍    | 80/148 [01:46<01:22,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▍    | 81/148 [01:48<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▍    | 81/148 [01:48<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▌    | 82/148 [01:49<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▌    | 82/148 [01:49<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m56%|█████▌    | 83/148 [01:50<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m56%|█████▌    | 83/148 [01:50<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 84/148 [01:51<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 84/148 [01:51<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 85/148 [01:53<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 85/148 [01:53<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[35m58%|█████▊    | 86/148 [01:54<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m58%|█████▊    | 86/148 [01:54<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 87/148 [01:55<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 87/148 [01:55<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 88/148 [01:56<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 88/148 [01:56<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m60%|██████    | 89/148 [01:57<01:11,  1.22s/it]\u001b[0m\n",
            "\u001b[34m60%|██████    | 89/148 [01:57<01:11,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████    | 90/148 [01:59<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████    | 90/148 [01:59<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████▏   | 91/148 [02:00<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████▏   | 91/148 [02:00<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m62%|██████▏   | 92/148 [02:01<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m62%|██████▏   | 92/148 [02:01<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m63%|██████▎   | 93/148 [02:02<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m63%|██████▎   | 93/148 [02:02<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▎   | 94/148 [02:04<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▎   | 94/148 [02:04<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▍   | 95/148 [02:05<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▍   | 95/148 [02:05<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m65%|██████▍   | 96/148 [02:06<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m65%|██████▍   | 96/148 [02:06<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 97/148 [02:07<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 97/148 [02:07<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 98/148 [02:08<01:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 98/148 [02:08<01:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m67%|██████▋   | 99/148 [02:10<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m67%|██████▋   | 99/148 [02:10<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 100/148 [02:11<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 100/148 [02:11<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 101/148 [02:12<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 101/148 [02:12<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m69%|██████▉   | 102/148 [02:13<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m69%|██████▉   | 102/148 [02:13<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|██████▉   | 103/148 [02:14<00:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|██████▉   | 103/148 [02:15<00:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|███████   | 104/148 [02:16<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|███████   | 104/148 [02:16<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m71%|███████   | 105/148 [02:17<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m71%|███████   | 105/148 [02:17<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 106/148 [02:18<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 106/148 [02:18<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 107/148 [02:19<00:50,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 107/148 [02:19<00:50,  1.22s/it]\u001b[0m\n",
            "\u001b[35m73%|███████▎  | 108/148 [02:21<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m73%|███████▎  | 108/148 [02:21<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▎  | 109/148 [02:22<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▎  | 109/148 [02:22<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▍  | 110/148 [02:23<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▍  | 110/148 [02:23<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m75%|███████▌  | 111/148 [02:24<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[34m75%|███████▌  | 111/148 [02:24<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▌  | 112/148 [02:25<00:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▌  | 112/148 [02:25<00:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▋  | 113/148 [02:27<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▋  | 113/148 [02:27<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m77%|███████▋  | 114/148 [02:28<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m77%|███████▋  | 114/148 [02:28<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 115/148 [02:29<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 115/148 [02:29<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 116/148 [02:30<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 116/148 [02:30<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[35m79%|███████▉  | 117/148 [02:32<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m79%|███████▉  | 117/148 [02:32<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|███████▉  | 118/148 [02:33<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|███████▉  | 118/148 [02:33<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|████████  | 119/148 [02:34<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|████████  | 119/148 [02:34<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m81%|████████  | 120/148 [02:35<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m81%|████████  | 120/148 [02:35<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 121/148 [02:36<00:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 121/148 [02:36<00:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 122/148 [02:38<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 122/148 [02:38<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m83%|████████▎ | 123/148 [02:39<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m83%|████████▎ | 123/148 [02:39<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 124/148 [02:40<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 124/148 [02:40<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 125/148 [02:41<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 125/148 [02:41<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[35m85%|████████▌ | 126/148 [02:43<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m85%|████████▌ | 126/148 [02:43<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▌ | 127/148 [02:44<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▌ | 127/148 [02:44<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▋ | 128/148 [02:45<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▋ | 128/148 [02:45<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m87%|████████▋ | 129/148 [02:46<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m87%|████████▋ | 129/148 [02:46<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m88%|████████▊ | 130/148 [02:47<00:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m88%|████████▊ | 130/148 [02:47<00:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▊ | 131/148 [02:49<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▊ | 131/148 [02:49<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▉ | 132/148 [02:50<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▉ | 132/148 [02:50<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m90%|████████▉ | 133/148 [02:51<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m90%|████████▉ | 133/148 [02:51<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 134/148 [02:52<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 134/148 [02:52<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 135/148 [02:54<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 135/148 [02:54<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m92%|█████████▏| 136/148 [02:55<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m92%|█████████▏| 136/148 [02:55<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 137/148 [02:56<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 137/148 [02:56<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 138/148 [02:57<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 138/148 [02:57<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m94%|█████████▍| 139/148 [02:58<00:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m94%|█████████▍| 139/148 [02:58<00:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▍| 140/148 [03:00<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▍| 140/148 [03:00<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▌| 141/148 [03:01<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▌| 141/148 [03:01<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m96%|█████████▌| 142/148 [03:02<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m96%|█████████▌| 142/148 [03:02<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 143/148 [03:03<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 143/148 [03:03<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 144/148 [03:05<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 144/148 [03:05<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m98%|█████████▊| 145/148 [03:06<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m98%|█████████▊| 145/148 [03:06<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▊| 146/148 [03:07<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▊| 146/148 [03:07<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▉| 147/148 [03:08<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▉| 147/148 [03:08<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m100%|██████████| 148/148 [03:09<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m100%|██████████| 148/148 [03:09<00:00,  1.28s/it]\u001b[0m\n",
            "\u001b[34m100%|██████████| 148/148 [03:09<00:00,  1.22s/it]#015100%|██████████| 148/148 [03:09<00:00,  1.28s/it]\u001b[0m\n",
            "\u001b[35mEpoch 1/3   Device 1   Train loss: 3.731   Validation loss: 2.368   Validation accuracy: 0.099\u001b[0m\n",
            "\u001b[35mINFO:__main__:Epoch 1/3   Device 1   Train loss: 3.731   Validation loss: 2.368   Validation accuracy: 0.099\u001b[0m\n",
            "\u001b[35m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[34mEpoch 1/3   Device 0   Train loss: 3.733   Validation loss: 2.369   Validation accuracy: 0.098\u001b[0m\n",
            "\u001b[34mINFO:__main__:Epoch 1/3   Device 0   Train loss: 3.733   Validation loss: 2.369   Validation accuracy: 0.098\u001b[0m\n",
            "\u001b[34m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[35m1%|          | 1/148 [00:01<02:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m1%|          | 1/148 [00:01<03:01,  1.24s/it]\u001b[0m\n",
            "\u001b[35m1%|▏         | 2/148 [00:02<02:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m1%|▏         | 2/148 [00:02<02:59,  1.23s/it]\u001b[0m\n",
            "\u001b[35m2%|▏         | 3/148 [00:03<02:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m2%|▏         | 3/148 [00:03<02:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 4/148 [00:04<02:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 4/148 [00:04<02:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 5/148 [00:06<02:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 5/148 [00:06<02:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m4%|▍         | 6/148 [00:07<02:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m4%|▍         | 6/148 [00:07<02:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m5%|▍         | 7/148 [00:08<02:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m5%|▍         | 7/148 [00:08<02:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m5%|▌         | 8/148 [00:09<02:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m5%|▌         | 8/148 [00:09<02:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m6%|▌         | 9/148 [00:10<02:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m6%|▌         | 9/148 [00:11<02:49,  1.22s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 10/148 [00:12<02:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 10/148 [00:12<02:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 11/148 [00:13<02:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 11/148 [00:13<02:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m8%|▊         | 12/148 [00:14<02:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m8%|▊         | 12/148 [00:14<02:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 13/148 [00:15<02:44,  1.22s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 13/148 [00:15<02:44,  1.22s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 14/148 [00:17<02:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 14/148 [00:17<02:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m10%|█         | 15/148 [00:18<02:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m10%|█         | 15/148 [00:18<02:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m11%|█         | 16/148 [00:19<02:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m11%|█         | 16/148 [00:19<02:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m11%|█▏        | 17/148 [00:20<02:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m11%|█▏        | 17/148 [00:20<02:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m12%|█▏        | 18/148 [00:21<02:38,  1.22s/it]\u001b[0m\n",
            "\u001b[34m12%|█▏        | 18/148 [00:22<02:38,  1.22s/it]\u001b[0m\n",
            "\u001b[35m13%|█▎        | 19/148 [00:23<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m13%|█▎        | 19/148 [00:23<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▎        | 20/148 [00:24<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▎        | 20/148 [00:24<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▍        | 21/148 [00:25<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▍        | 21/148 [00:25<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m15%|█▍        | 22/148 [00:26<02:33,  1.22s/it]\u001b[0m\n",
            "\u001b[34m15%|█▍        | 22/148 [00:26<02:33,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 23/148 [00:28<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 23/148 [00:28<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 24/148 [00:29<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 24/148 [00:29<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m17%|█▋        | 25/148 [00:30<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m17%|█▋        | 25/148 [00:30<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 26/148 [00:31<02:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 26/148 [00:31<02:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 27/148 [00:32<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m19%|█▉        | 28/148 [00:34<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|█▉        | 29/148 [00:35<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|██        | 30/148 [00:36<02:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m21%|██        | 31/148 [00:37<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 27/148 [00:33<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m19%|█▉        | 28/148 [00:34<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|█▉        | 29/148 [00:35<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|██        | 30/148 [00:36<02:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m21%|██        | 31/148 [00:37<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 32/148 [00:39<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 32/148 [00:39<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 33/148 [00:40<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 33/148 [00:40<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m23%|██▎       | 34/148 [00:41<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m23%|██▎       | 34/148 [00:41<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▎       | 35/148 [00:42<02:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▎       | 35/148 [00:42<02:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▍       | 36/148 [00:44<02:17,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▍       | 36/148 [00:44<02:17,  1.22s/it]\u001b[0m\n",
            "\u001b[35m25%|██▌       | 37/148 [00:45<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m25%|██▌       | 37/148 [00:45<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▌       | 38/148 [00:46<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▌       | 38/148 [00:46<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▋       | 39/148 [00:47<02:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▋       | 39/148 [00:47<02:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m27%|██▋       | 40/148 [00:48<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m27%|██▋       | 40/148 [00:48<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 41/148 [00:50<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 41/148 [00:50<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 42/148 [00:51<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 42/148 [00:51<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m29%|██▉       | 43/148 [00:52<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m29%|██▉       | 43/148 [00:52<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|██▉       | 44/148 [00:53<02:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|██▉       | 44/148 [00:53<02:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|███       | 45/148 [00:55<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|███       | 45/148 [00:55<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m31%|███       | 46/148 [00:56<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m31%|███       | 46/148 [00:56<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 47/148 [00:57<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 47/148 [00:57<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 48/148 [00:58<02:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 48/148 [00:58<02:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m33%|███▎      | 49/148 [00:59<02:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m33%|███▎      | 49/148 [00:59<02:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 50/148 [01:01<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 50/148 [01:01<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 51/148 [01:02<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 51/148 [01:02<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m35%|███▌      | 52/148 [01:03<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m35%|███▌      | 52/148 [01:03<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▌      | 53/148 [01:04<01:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▌      | 53/148 [01:04<01:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▋      | 54/148 [01:06<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▋      | 54/148 [01:06<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m37%|███▋      | 55/148 [01:07<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m37%|███▋      | 55/148 [01:07<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m38%|███▊      | 56/148 [01:08<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m38%|███▊      | 56/148 [01:08<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▊      | 57/148 [01:09<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▊      | 57/148 [01:09<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▉      | 58/148 [01:10<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▉      | 58/148 [01:10<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[35m40%|███▉      | 59/148 [01:12<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m40%|███▉      | 59/148 [01:12<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 60/148 [01:13<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 60/148 [01:13<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 61/148 [01:14<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 61/148 [01:14<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m42%|████▏     | 62/148 [01:15<01:45,  1.22s/it]\u001b[0m\n",
            "\u001b[34m42%|████▏     | 62/148 [01:15<01:45,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 63/148 [01:17<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 63/148 [01:17<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 64/148 [01:18<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 64/148 [01:18<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m44%|████▍     | 65/148 [01:19<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m44%|████▍     | 65/148 [01:19<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▍     | 66/148 [01:20<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▍     | 66/148 [01:20<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▌     | 67/148 [01:21<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▌     | 67/148 [01:21<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[35m46%|████▌     | 68/148 [01:23<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m46%|████▌     | 68/148 [01:23<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 69/148 [01:24<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 69/148 [01:24<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 70/148 [01:25<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 70/148 [01:25<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m48%|████▊     | 71/148 [01:26<01:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m48%|████▊     | 71/148 [01:26<01:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▊     | 72/148 [01:28<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▊     | 72/148 [01:28<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▉     | 73/148 [01:29<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▉     | 73/148 [01:29<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m50%|█████     | 74/148 [01:30<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m50%|█████     | 74/148 [01:30<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████     | 75/148 [01:31<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████     | 75/148 [01:31<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████▏    | 76/148 [01:32<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████▏    | 76/148 [01:32<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m52%|█████▏    | 77/148 [01:34<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m52%|█████▏    | 77/148 [01:34<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 78/148 [01:35<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 78/148 [01:35<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 79/148 [01:36<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 79/148 [01:36<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m54%|█████▍    | 80/148 [01:37<01:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m54%|█████▍    | 80/148 [01:37<01:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▍    | 81/148 [01:39<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▍    | 81/148 [01:38<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▌    | 82/148 [01:40<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▌    | 82/148 [01:40<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m56%|█████▌    | 83/148 [01:41<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m56%|█████▌    | 83/148 [01:41<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 84/148 [01:42<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 84/148 [01:42<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 85/148 [01:43<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 85/148 [01:43<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[35m58%|█████▊    | 86/148 [01:45<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m58%|█████▊    | 86/148 [01:45<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 87/148 [01:46<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 87/148 [01:46<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 88/148 [01:47<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 88/148 [01:47<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m60%|██████    | 89/148 [01:48<01:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m60%|██████    | 89/148 [01:48<01:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████    | 90/148 [01:49<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████    | 90/148 [01:50<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████▏   | 91/148 [01:51<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m62%|██████▏   | 92/148 [01:52<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m63%|██████▎   | 93/148 [01:53<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▎   | 94/148 [01:54<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████▏   | 91/148 [01:51<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m62%|██████▏   | 92/148 [01:52<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m63%|██████▎   | 93/148 [01:53<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▎   | 94/148 [01:54<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▍   | 95/148 [01:56<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▍   | 95/148 [01:56<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m65%|██████▍   | 96/148 [01:57<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m65%|██████▍   | 96/148 [01:57<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 97/148 [01:58<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 97/148 [01:58<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 98/148 [01:59<01:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 98/148 [01:59<01:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m67%|██████▋   | 99/148 [02:00<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m67%|██████▋   | 99/148 [02:01<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 100/148 [02:02<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 100/148 [02:02<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 101/148 [02:03<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 101/148 [02:03<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m69%|██████▉   | 102/148 [02:04<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m69%|██████▉   | 102/148 [02:04<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|██████▉   | 103/148 [02:05<00:55,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|██████▉   | 103/148 [02:05<00:55,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|███████   | 104/148 [02:07<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|███████   | 104/148 [02:07<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m71%|███████   | 105/148 [02:08<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m71%|███████   | 105/148 [02:08<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 106/148 [02:09<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 106/148 [02:09<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 107/148 [02:10<00:50,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 107/148 [02:10<00:50,  1.22s/it]\u001b[0m\n",
            "\u001b[35m73%|███████▎  | 108/148 [02:11<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m73%|███████▎  | 108/148 [02:12<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▎  | 109/148 [02:13<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▎  | 109/148 [02:13<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▍  | 110/148 [02:14<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▍  | 110/148 [02:14<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m75%|███████▌  | 111/148 [02:15<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[34m75%|███████▌  | 111/148 [02:15<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▌  | 112/148 [02:16<00:44,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▌  | 112/148 [02:16<00:44,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▋  | 113/148 [02:18<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▋  | 113/148 [02:18<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m77%|███████▋  | 114/148 [02:19<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m77%|███████▋  | 114/148 [02:19<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 115/148 [02:20<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 115/148 [02:20<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 116/148 [02:21<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 116/148 [02:21<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[35m79%|███████▉  | 117/148 [02:22<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m79%|███████▉  | 117/148 [02:23<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|███████▉  | 118/148 [02:24<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|███████▉  | 118/148 [02:24<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|████████  | 119/148 [02:25<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|████████  | 119/148 [02:25<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m81%|████████  | 120/148 [02:26<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m81%|████████  | 120/148 [02:26<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 121/148 [02:27<00:33,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 121/148 [02:27<00:33,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 122/148 [02:29<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 122/148 [02:29<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m83%|████████▎ | 123/148 [02:30<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m83%|████████▎ | 123/148 [02:30<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 124/148 [02:31<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 124/148 [02:31<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 125/148 [02:32<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 125/148 [02:32<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[35m85%|████████▌ | 126/148 [02:33<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m85%|████████▌ | 126/148 [02:34<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▌ | 127/148 [02:35<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▌ | 127/148 [02:35<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▋ | 128/148 [02:36<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▋ | 128/148 [02:36<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m87%|████████▋ | 129/148 [02:37<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m87%|████████▋ | 129/148 [02:37<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m88%|████████▊ | 130/148 [02:38<00:22,  1.22s/it]\u001b[0m\n",
            "\u001b[34m88%|████████▊ | 130/148 [02:38<00:22,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▊ | 131/148 [02:40<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▊ | 131/148 [02:40<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▉ | 132/148 [02:41<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▉ | 132/148 [02:41<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m90%|████████▉ | 133/148 [02:42<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m90%|████████▉ | 133/148 [02:42<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 134/148 [02:43<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 134/148 [02:43<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 135/148 [02:44<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 135/148 [02:45<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m92%|█████████▏| 136/148 [02:46<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m92%|█████████▏| 136/148 [02:46<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 137/148 [02:47<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 137/148 [02:47<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 138/148 [02:48<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 138/148 [02:48<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m94%|█████████▍| 139/148 [02:49<00:11,  1.22s/it]\u001b[0m\n",
            "\u001b[34m94%|█████████▍| 139/148 [02:49<00:11,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▍| 140/148 [02:51<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▍| 140/148 [02:51<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▌| 141/148 [02:52<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▌| 141/148 [02:52<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m96%|█████████▌| 142/148 [02:53<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m96%|█████████▌| 142/148 [02:53<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 143/148 [02:54<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 143/148 [02:54<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 144/148 [02:55<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 144/148 [02:56<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m98%|█████████▊| 145/148 [02:57<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▊| 146/148 [02:58<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▉| 147/148 [02:59<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m100%|██████████| 148/148 [03:00<00:00,  1.22s/it]#015100%|██████████| 148/148 [03:00<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m98%|█████████▊| 145/148 [02:57<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▊| 146/148 [02:58<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▉| 147/148 [02:59<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m100%|██████████| 148/148 [03:00<00:00,  1.22s/it]#015100%|██████████| 148/148 [03:00<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35mEpoch 2/3   Device 1   Train loss: 2.364   Validation loss: 2.336   Validation accuracy: 0.099\u001b[0m\n",
            "\u001b[35mINFO:__main__:Epoch 2/3   Device 1   Train loss: 2.364   Validation loss: 2.336   Validation accuracy: 0.099\u001b[0m\n",
            "\u001b[35m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[34mEpoch 2/3   Device 0   Train loss: 2.364   Validation loss: 2.336   Validation accuracy: 0.098\u001b[0m\n",
            "\u001b[34mINFO:__main__:Epoch 2/3   Device 0   Train loss: 2.364   Validation loss: 2.336   Validation accuracy: 0.098\u001b[0m\n",
            "\u001b[34m0%|          | 0/148 [00:00<?, ?it/s]\u001b[0m\n",
            "\u001b[35m1%|          | 1/148 [00:01<03:00,  1.22s/it]\u001b[0m\n",
            "\u001b[34m1%|          | 1/148 [00:01<03:06,  1.27s/it]\u001b[0m\n",
            "\u001b[35m1%|▏         | 2/148 [00:02<02:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m1%|▏         | 2/148 [00:02<03:01,  1.24s/it]\u001b[0m\n",
            "\u001b[35m2%|▏         | 3/148 [00:03<02:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m2%|▏         | 3/148 [00:03<02:58,  1.23s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 4/148 [00:04<02:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 4/148 [00:04<02:57,  1.23s/it]\u001b[0m\n",
            "\u001b[35m3%|▎         | 5/148 [00:06<02:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m3%|▎         | 5/148 [00:06<02:55,  1.23s/it]\u001b[0m\n",
            "\u001b[34m4%|▍         | 6/148 [00:07<02:54,  1.23s/it]\u001b[0m\n",
            "\u001b[35m4%|▍         | 6/148 [00:07<02:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m5%|▍         | 7/148 [00:08<02:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m5%|▍         | 7/148 [00:08<02:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m5%|▌         | 8/148 [00:09<02:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m5%|▌         | 8/148 [00:09<02:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m6%|▌         | 9/148 [00:11<02:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m6%|▌         | 9/148 [00:11<02:50,  1.22s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 10/148 [00:12<02:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 10/148 [00:12<02:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m7%|▋         | 11/148 [00:13<02:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m7%|▋         | 11/148 [00:13<02:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m8%|▊         | 12/148 [00:14<02:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m8%|▊         | 12/148 [00:14<02:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 13/148 [00:15<02:45,  1.22s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 13/148 [00:15<02:45,  1.22s/it]\u001b[0m\n",
            "\u001b[35m9%|▉         | 14/148 [00:17<02:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m9%|▉         | 14/148 [00:17<02:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m10%|█         | 15/148 [00:18<02:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m10%|█         | 15/148 [00:18<02:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m11%|█         | 16/148 [00:19<02:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m11%|█         | 16/148 [00:19<02:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m11%|█▏        | 17/148 [00:20<02:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m11%|█▏        | 17/148 [00:20<02:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m12%|█▏        | 18/148 [00:22<02:38,  1.22s/it]\u001b[0m\n",
            "\u001b[34m12%|█▏        | 18/148 [00:22<02:38,  1.22s/it]\u001b[0m\n",
            "\u001b[35m13%|█▎        | 19/148 [00:23<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m13%|█▎        | 19/148 [00:23<02:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▎        | 20/148 [00:24<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▎        | 20/148 [00:24<02:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m14%|█▍        | 21/148 [00:25<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m14%|█▍        | 21/148 [00:25<02:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m15%|█▍        | 22/148 [00:26<02:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m15%|█▍        | 22/148 [00:26<02:34,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 23/148 [00:28<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 23/148 [00:28<02:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m16%|█▌        | 24/148 [00:29<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m16%|█▌        | 24/148 [00:29<02:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m17%|█▋        | 25/148 [00:30<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m17%|█▋        | 25/148 [00:30<02:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 26/148 [00:31<02:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 26/148 [00:31<02:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m18%|█▊        | 27/148 [00:33<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m18%|█▊        | 27/148 [00:33<02:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m19%|█▉        | 28/148 [00:34<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m19%|█▉        | 28/148 [00:34<02:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|█▉        | 29/148 [00:35<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|█▉        | 29/148 [00:35<02:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m20%|██        | 30/148 [00:36<02:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m20%|██        | 30/148 [00:36<02:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m21%|██        | 31/148 [00:37<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m21%|██        | 31/148 [00:37<02:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 32/148 [00:39<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 32/148 [00:39<02:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m22%|██▏       | 33/148 [00:40<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m22%|██▏       | 33/148 [00:40<02:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m23%|██▎       | 34/148 [00:41<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m23%|██▎       | 34/148 [00:41<02:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▎       | 35/148 [00:42<02:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▎       | 35/148 [00:42<02:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m24%|██▍       | 36/148 [00:44<02:16,  1.22s/it]\u001b[0m\n",
            "\u001b[34m24%|██▍       | 36/148 [00:44<02:16,  1.22s/it]\u001b[0m\n",
            "\u001b[35m25%|██▌       | 37/148 [00:45<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m25%|██▌       | 37/148 [00:45<02:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▌       | 38/148 [00:46<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▌       | 38/148 [00:46<02:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m26%|██▋       | 39/148 [00:47<02:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m26%|██▋       | 39/148 [00:47<02:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m27%|██▋       | 40/148 [00:48<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m27%|██▋       | 40/148 [00:48<02:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 41/148 [00:50<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 41/148 [00:50<02:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m28%|██▊       | 42/148 [00:51<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m28%|██▊       | 42/148 [00:51<02:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m29%|██▉       | 43/148 [00:52<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m29%|██▉       | 43/148 [00:52<02:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|██▉       | 44/148 [00:53<02:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|██▉       | 44/148 [00:53<02:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m30%|███       | 45/148 [00:55<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[34m30%|███       | 45/148 [00:55<02:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m31%|███       | 46/148 [00:56<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m31%|███       | 46/148 [00:56<02:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 47/148 [00:57<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m32%|███▏      | 48/148 [00:58<02:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m33%|███▎      | 49/148 [00:59<02:00,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 50/148 [01:01<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m34%|███▍      | 51/148 [01:02<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 47/148 [00:57<02:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m32%|███▏      | 48/148 [00:58<02:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m33%|███▎      | 49/148 [00:59<02:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 50/148 [01:01<01:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m34%|███▍      | 51/148 [01:02<01:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m35%|███▌      | 52/148 [01:03<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m35%|███▌      | 52/148 [01:03<01:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▌      | 53/148 [01:04<01:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▌      | 53/148 [01:04<01:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m36%|███▋      | 54/148 [01:06<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m36%|███▋      | 54/148 [01:06<01:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m37%|███▋      | 55/148 [01:07<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m37%|███▋      | 55/148 [01:07<01:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m38%|███▊      | 56/148 [01:08<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m38%|███▊      | 56/148 [01:08<01:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▊      | 57/148 [01:09<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▊      | 57/148 [01:09<01:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m39%|███▉      | 58/148 [01:10<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m39%|███▉      | 58/148 [01:10<01:49,  1.22s/it]\u001b[0m\n",
            "\u001b[35m40%|███▉      | 59/148 [01:12<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m40%|███▉      | 59/148 [01:12<01:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 60/148 [01:13<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 60/148 [01:13<01:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m41%|████      | 61/148 [01:14<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m42%|████▏     | 62/148 [01:15<01:44,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 63/148 [01:17<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m43%|████▎     | 64/148 [01:18<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m41%|████      | 61/148 [01:14<01:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m42%|████▏     | 62/148 [01:15<01:44,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 63/148 [01:16<01:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m43%|████▎     | 64/148 [01:18<01:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m44%|████▍     | 65/148 [01:19<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m44%|████▍     | 65/148 [01:19<01:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▍     | 66/148 [01:20<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▍     | 66/148 [01:20<01:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m45%|████▌     | 67/148 [01:21<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[34m45%|████▌     | 67/148 [01:21<01:38,  1.22s/it]\u001b[0m\n",
            "\u001b[35m46%|████▌     | 68/148 [01:23<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m46%|████▌     | 68/148 [01:23<01:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 69/148 [01:24<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 69/148 [01:24<01:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m47%|████▋     | 70/148 [01:25<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m47%|████▋     | 70/148 [01:25<01:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m48%|████▊     | 71/148 [01:26<01:33,  1.22s/it]\u001b[0m\n",
            "\u001b[34m48%|████▊     | 71/148 [01:26<01:33,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▊     | 72/148 [01:27<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▊     | 72/148 [01:28<01:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m49%|████▉     | 73/148 [01:29<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m49%|████▉     | 73/148 [01:29<01:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m50%|█████     | 74/148 [01:30<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m50%|█████     | 74/148 [01:30<01:30,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████     | 75/148 [01:31<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████     | 75/148 [01:31<01:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m51%|█████▏    | 76/148 [01:32<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[34m51%|█████▏    | 76/148 [01:32<01:27,  1.22s/it]\u001b[0m\n",
            "\u001b[35m52%|█████▏    | 77/148 [01:34<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m52%|█████▏    | 77/148 [01:34<01:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 78/148 [01:35<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 78/148 [01:35<01:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m53%|█████▎    | 79/148 [01:36<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m53%|█████▎    | 79/148 [01:36<01:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m54%|█████▍    | 80/148 [01:37<01:22,  1.22s/it]\u001b[0m\n",
            "\u001b[34m54%|█████▍    | 80/148 [01:37<01:22,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▍    | 81/148 [01:38<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▍    | 81/148 [01:38<01:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m55%|█████▌    | 82/148 [01:40<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m55%|█████▌    | 82/148 [01:40<01:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m56%|█████▌    | 83/148 [01:41<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m56%|█████▌    | 83/148 [01:41<01:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 84/148 [01:42<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 84/148 [01:42<01:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m57%|█████▋    | 85/148 [01:43<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[35m58%|█████▊    | 86/148 [01:45<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 87/148 [01:46<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m57%|█████▋    | 85/148 [01:43<01:16,  1.22s/it]\u001b[0m\n",
            "\u001b[34m58%|█████▊    | 86/148 [01:45<01:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 87/148 [01:46<01:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m59%|█████▉    | 88/148 [01:47<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m59%|█████▉    | 88/148 [01:47<01:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m60%|██████    | 89/148 [01:48<01:11,  1.22s/it]\u001b[0m\n",
            "\u001b[34m60%|██████    | 89/148 [01:48<01:11,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████    | 90/148 [01:49<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████    | 90/148 [01:49<01:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m61%|██████▏   | 91/148 [01:51<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m61%|██████▏   | 91/148 [01:51<01:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m62%|██████▏   | 92/148 [01:52<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m62%|██████▏   | 92/148 [01:52<01:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m63%|██████▎   | 93/148 [01:53<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m63%|██████▎   | 93/148 [01:53<01:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▎   | 94/148 [01:54<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▎   | 94/148 [01:54<01:05,  1.22s/it]\u001b[0m\n",
            "\u001b[35m64%|██████▍   | 95/148 [01:56<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m64%|██████▍   | 95/148 [01:56<01:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m65%|██████▍   | 96/148 [01:57<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m65%|██████▍   | 96/148 [01:57<01:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 97/148 [01:58<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 97/148 [01:58<01:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m66%|██████▌   | 98/148 [01:59<01:00,  1.22s/it]\u001b[0m\n",
            "\u001b[34m66%|██████▌   | 98/148 [01:59<01:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m67%|██████▋   | 99/148 [02:00<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[34m67%|██████▋   | 99/148 [02:00<00:59,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 100/148 [02:02<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 100/148 [02:02<00:58,  1.22s/it]\u001b[0m\n",
            "\u001b[35m68%|██████▊   | 101/148 [02:03<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[34m68%|██████▊   | 101/148 [02:03<00:57,  1.22s/it]\u001b[0m\n",
            "\u001b[35m69%|██████▉   | 102/148 [02:04<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[34m69%|██████▉   | 102/148 [02:04<00:56,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|██████▉   | 103/148 [02:05<00:54,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|██████▉   | 103/148 [02:05<00:54,  1.22s/it]\u001b[0m\n",
            "\u001b[35m70%|███████   | 104/148 [02:07<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[34m70%|███████   | 104/148 [02:07<00:53,  1.22s/it]\u001b[0m\n",
            "\u001b[35m71%|███████   | 105/148 [02:08<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[34m71%|███████   | 105/148 [02:08<00:52,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 106/148 [02:09<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 106/148 [02:09<00:51,  1.22s/it]\u001b[0m\n",
            "\u001b[35m72%|███████▏  | 107/148 [02:10<00:49,  1.22s/it]\u001b[0m\n",
            "\u001b[34m72%|███████▏  | 107/148 [02:10<00:49,  1.22s/it]\u001b[0m\n",
            "\u001b[35m73%|███████▎  | 108/148 [02:11<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[34m73%|███████▎  | 108/148 [02:11<00:48,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▎  | 109/148 [02:13<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▎  | 109/148 [02:13<00:47,  1.22s/it]\u001b[0m\n",
            "\u001b[35m74%|███████▍  | 110/148 [02:14<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[34m74%|███████▍  | 110/148 [02:14<00:46,  1.22s/it]\u001b[0m\n",
            "\u001b[35m75%|███████▌  | 111/148 [02:15<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[34m75%|███████▌  | 111/148 [02:15<00:45,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▌  | 112/148 [02:16<00:43,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▌  | 112/148 [02:16<00:43,  1.22s/it]\u001b[0m\n",
            "\u001b[35m76%|███████▋  | 113/148 [02:17<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[34m76%|███████▋  | 113/148 [02:18<00:42,  1.22s/it]\u001b[0m\n",
            "\u001b[35m77%|███████▋  | 114/148 [02:19<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[34m77%|███████▋  | 114/148 [02:19<00:41,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 115/148 [02:20<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 115/148 [02:20<00:40,  1.22s/it]\u001b[0m\n",
            "\u001b[35m78%|███████▊  | 116/148 [02:21<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[34m78%|███████▊  | 116/148 [02:21<00:39,  1.22s/it]\u001b[0m\n",
            "\u001b[35m79%|███████▉  | 117/148 [02:22<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[34m79%|███████▉  | 117/148 [02:22<00:37,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|███████▉  | 118/148 [02:24<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|███████▉  | 118/148 [02:24<00:36,  1.22s/it]\u001b[0m\n",
            "\u001b[35m80%|████████  | 119/148 [02:25<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[34m80%|████████  | 119/148 [02:25<00:35,  1.22s/it]\u001b[0m\n",
            "\u001b[35m81%|████████  | 120/148 [02:26<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[34m81%|████████  | 120/148 [02:26<00:34,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 121/148 [02:27<00:32,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 121/148 [02:27<00:32,  1.22s/it]\u001b[0m\n",
            "\u001b[35m82%|████████▏ | 122/148 [02:28<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[34m82%|████████▏ | 122/148 [02:28<00:31,  1.22s/it]\u001b[0m\n",
            "\u001b[35m83%|████████▎ | 123/148 [02:30<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m83%|████████▎ | 123/148 [02:30<00:30,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 124/148 [02:31<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 124/148 [02:31<00:29,  1.22s/it]\u001b[0m\n",
            "\u001b[35m84%|████████▍ | 125/148 [02:32<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[34m84%|████████▍ | 125/148 [02:32<00:28,  1.22s/it]\u001b[0m\n",
            "\u001b[35m85%|████████▌ | 126/148 [02:33<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[34m85%|████████▌ | 126/148 [02:33<00:26,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▌ | 127/148 [02:35<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▌ | 127/148 [02:35<00:25,  1.22s/it]\u001b[0m\n",
            "\u001b[35m86%|████████▋ | 128/148 [02:36<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[34m86%|████████▋ | 128/148 [02:36<00:24,  1.22s/it]\u001b[0m\n",
            "\u001b[35m87%|████████▋ | 129/148 [02:37<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[34m87%|████████▋ | 129/148 [02:37<00:23,  1.22s/it]\u001b[0m\n",
            "\u001b[35m88%|████████▊ | 130/148 [02:38<00:21,  1.22s/it]\u001b[0m\n",
            "\u001b[34m88%|████████▊ | 130/148 [02:38<00:21,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▊ | 131/148 [02:39<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▊ | 131/148 [02:39<00:20,  1.22s/it]\u001b[0m\n",
            "\u001b[35m89%|████████▉ | 132/148 [02:41<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[34m89%|████████▉ | 132/148 [02:41<00:19,  1.22s/it]\u001b[0m\n",
            "\u001b[35m90%|████████▉ | 133/148 [02:42<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[34m90%|████████▉ | 133/148 [02:42<00:18,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 134/148 [02:43<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 134/148 [02:43<00:17,  1.22s/it]\u001b[0m\n",
            "\u001b[35m91%|█████████ | 135/148 [02:44<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[34m91%|█████████ | 135/148 [02:44<00:15,  1.22s/it]\u001b[0m\n",
            "\u001b[35m92%|█████████▏| 136/148 [02:46<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[34m92%|█████████▏| 136/148 [02:46<00:14,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 137/148 [02:47<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 137/148 [02:47<00:13,  1.22s/it]\u001b[0m\n",
            "\u001b[35m93%|█████████▎| 138/148 [02:48<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[34m93%|█████████▎| 138/148 [02:48<00:12,  1.22s/it]\u001b[0m\n",
            "\u001b[35m94%|█████████▍| 139/148 [02:49<00:10,  1.22s/it]\u001b[0m\n",
            "\u001b[34m94%|█████████▍| 139/148 [02:49<00:10,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▍| 140/148 [02:50<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▍| 140/148 [02:50<00:09,  1.22s/it]\u001b[0m\n",
            "\u001b[35m95%|█████████▌| 141/148 [02:52<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[34m95%|█████████▌| 141/148 [02:52<00:08,  1.22s/it]\u001b[0m\n",
            "\u001b[35m96%|█████████▌| 142/148 [02:53<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[34m96%|█████████▌| 142/148 [02:53<00:07,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 143/148 [02:54<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 143/148 [02:54<00:06,  1.22s/it]\u001b[0m\n",
            "\u001b[35m97%|█████████▋| 144/148 [02:55<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[34m97%|█████████▋| 144/148 [02:55<00:04,  1.22s/it]\u001b[0m\n",
            "\u001b[35m98%|█████████▊| 145/148 [02:57<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[34m98%|█████████▊| 145/148 [02:57<00:03,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▊| 146/148 [02:58<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▊| 146/148 [02:58<00:02,  1.22s/it]\u001b[0m\n",
            "\u001b[35m99%|█████████▉| 147/148 [02:59<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[34m99%|█████████▉| 147/148 [02:59<00:01,  1.22s/it]\u001b[0m\n",
            "\u001b[35m100%|██████████| 148/148 [03:00<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35m100%|██████████| 148/148 [03:00<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[34m100%|██████████| 148/148 [03:00<00:00,  1.22s/it]#015100%|██████████| 148/148 [03:00<00:00,  1.22s/it]\u001b[0m\n",
            "\u001b[35mEpoch 3/3   Device 1   Train loss: 2.353   Validation loss: 2.328   Validation accuracy: 0.090\u001b[0m\n",
            "\u001b[35mINFO:__main__:Epoch 3/3   Device 1   Train loss: 2.353   Validation loss: 2.328   Validation accuracy: 0.090\u001b[0m\n",
            "\u001b[35m--- Timer summary -----------------------------------------------\n",
            "  Event                          |  Count | Average time |  Frac.\u001b[0m\n",
            "\u001b[35m- backward_epoch0_rank1          |    148 |     1.03099s |  83.7%\u001b[0m\n",
            "\u001b[35m- backward_epoch1_rank1          |    148 |     1.01892s |  83.5%\u001b[0m\n",
            "\u001b[35m- backward_epoch2_rank1          |    148 |     1.01864s |  83.5%\u001b[0m\n",
            "\u001b[35m- forward_epoch0_rank1           |    148 |     0.11834s |   9.3%\u001b[0m\n",
            "\u001b[35m- forward_epoch1_rank1           |    148 |     0.06838s |   5.6%\u001b[0m\n",
            "\u001b[35m- forward_epoch2_rank1           |    148 |     0.06775s |   5.6%\u001b[0m\n",
            "\u001b[35m- pytorch.all_reduce             |   4434 |     0.00894s |   7.1%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch0_rank1         |      1 |   189.91591s | 100.0%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch1_rank1         |      1 |   180.88699s | 100.0%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch2_rank1         |      1 |   180.65596s | 100.0%\u001b[0m\n",
            "\u001b[35m-----------------------------------------------------------------\u001b[0m\n",
            "\u001b[35mINFO:__main__:--- Timer summary -----------------------------------------------\n",
            "  Event                          |  Count | Average time |  Frac.\u001b[0m\n",
            "\u001b[35m- backward_epoch0_rank1          |    148 |     1.03099s |  83.7%\u001b[0m\n",
            "\u001b[35m- backward_epoch1_rank1          |    148 |     1.01892s |  83.5%\u001b[0m\n",
            "\u001b[35m- backward_epoch2_rank1          |    148 |     1.01864s |  83.5%\u001b[0m\n",
            "\u001b[35m- forward_epoch0_rank1           |    148 |     0.11834s |   9.3%\u001b[0m\n",
            "\u001b[35m- forward_epoch1_rank1           |    148 |     0.06838s |   5.6%\u001b[0m\n",
            "\u001b[35m- forward_epoch2_rank1           |    148 |     0.06775s |   5.6%\u001b[0m\n",
            "\u001b[35m- pytorch.all_reduce             |   4434 |     0.00894s |   7.1%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch0_rank1         |      1 |   189.91591s | 100.0%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch1_rank1         |      1 |   180.88699s | 100.0%\u001b[0m\n",
            "\u001b[35m- trainloop_epoch2_rank1         |      1 |   180.65596s | 100.0%\u001b[0m\n",
            "\u001b[35m-----------------------------------------------------------------\u001b[0m\n",
            "\u001b[34mEpoch 3/3   Device 0   Train loss: 2.356   Validation loss: 2.328   Validation accuracy: 0.090\u001b[0m\n",
            "\u001b[34mINFO:__main__:Epoch 3/3   Device 0   Train loss: 2.356   Validation loss: 2.328   Validation accuracy: 0.090\u001b[0m\n",
            "\u001b[34m--- Timer summary -----------------------------------------------\n",
            "  Event                          |  Count | Average time |  Frac.\u001b[0m\n",
            "\u001b[34m- backward_epoch0_rank0          |    148 |     1.02605s |  83.7%\u001b[0m\n",
            "\u001b[34m- backward_epoch1_rank0          |    148 |     1.01942s |  83.5%\u001b[0m\n",
            "\u001b[34m- backward_epoch2_rank0          |    148 |     1.01908s |  83.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch0_rank0           |    148 |     0.12307s |   9.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch1_rank0           |    148 |     0.06801s |   5.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch2_rank0           |    148 |     0.06753s |   5.6%\u001b[0m\n",
            "\u001b[34m- pytorch.all_reduce             |   4434 |     0.00894s |   7.1%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch0_rank0         |      1 |   189.91961s | 100.0%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch1_rank0         |      1 |   180.89884s | 100.0%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch2_rank0         |      1 |   180.69848s | 100.0%\u001b[0m\n",
            "\u001b[34m-----------------------------------------------------------------\u001b[0m\n",
            "\u001b[34mINFO:__main__:--- Timer summary -----------------------------------------------\n",
            "  Event                          |  Count | Average time |  Frac.\u001b[0m\n",
            "\u001b[34m- backward_epoch0_rank0          |    148 |     1.02605s |  83.7%\u001b[0m\n",
            "\u001b[34m- backward_epoch1_rank0          |    148 |     1.01942s |  83.5%\u001b[0m\n",
            "\u001b[34m- backward_epoch2_rank0          |    148 |     1.01908s |  83.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch0_rank0           |    148 |     0.12307s |   9.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch1_rank0           |    148 |     0.06801s |   5.6%\u001b[0m\n",
            "\u001b[34m- forward_epoch2_rank0           |    148 |     0.06753s |   5.6%\u001b[0m\n",
            "\u001b[34m- pytorch.all_reduce             |   4434 |     0.00894s |   7.1%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch0_rank0         |      1 |   189.91961s | 100.0%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch1_rank0         |      1 |   180.89884s | 100.0%\u001b[0m\n",
            "\u001b[34m- trainloop_epoch2_rank0         |      1 |   180.69848s | 100.0%\u001b[0m\n",
            "\u001b[34m-----------------------------------------------------------------\u001b[0m\n",
            "\u001b[35m2022-12-05 21:51:25,003 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
            "\u001b[35m2022-12-05 21:51:25,003 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
            "\u001b[35m2022-12-05 21:51:25,004 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
            "\u001b[34m2022-12-05 21:51:25,163 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
            "\u001b[34m2022-12-05 21:51:25,163 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
            "\u001b[34m2022-12-05 21:51:25,164 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
            "\n",
            "2022-12-05 21:51:43 Uploading - Uploading generated training model\n",
            "2022-12-05 21:51:43 Completed - Training job completed\n",
            "Training seconds: 1812\n",
            "Billable seconds: 1812\n"
          ]
        }
      ],
      "source": [
        "pt_estimator.fit(\"s3://cs243-egg/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZZ7dA5g2Nbx"
      },
      "source": [
        "## Host\n",
        "### Create endpoint\n",
        "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
        "\n",
        "As mentioned above we have implementation of `model_fn` in the `mnist.py` script that is required. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
        "\n",
        "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`. Here we will deploy the model to a single ```ml.m4.xlarge``` instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0QkysiM2Nbx",
        "outputId": "e7683c22-805d-4cc4-ba87-492b246dc015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------!"
          ]
        }
      ],
      "source": [
        "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvdAH0Qc2Nby"
      },
      "source": [
        "### Evaluate\n",
        "\n",
        "You can use the test images to evalute the endpoint. The accuracy of the model depends on how many it is trained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIRZsR_h2Nby",
        "outputId": "d59eb82e-f57b-4af4-91ca-7fefa2caf421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\n",
            "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\n",
            "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "!ls data/MNIST/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu64Tl6h2Nbz",
        "outputId": "0cd1e3e8-baff-4dd2-b6c6-4745c53e38a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "data_dir = \"data/MNIST/raw\"\n",
        "with gzip.open(os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"rb\") as f:\n",
        "    images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28).astype(np.float32)\n",
        "\n",
        "mask = random.sample(range(len(images)), 16)  # randomly select some of the test images\n",
        "mask = np.array(mask, dtype=np.int)\n",
        "data = images[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY7AAqXD2Nbz",
        "outputId": "33107cfc-8254-4ba9-b470-49b3ddbce576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw prediction result:\n",
            "[[ -487.13415527  -206.11022949  -421.78015137  -318.37774658\n",
            "   -303.48300171  -195.57843018  -397.75170898  -310.17514038\n",
            "      0.          -206.61590576]\n",
            " [ -874.51171875 -1090.3873291   -678.98632812 -1250.96826172\n",
            "   -622.22998047  -647.68695068     0.         -1445.21044922\n",
            "   -858.81274414  -994.20300293]\n",
            " [ -474.9256897  -1307.36303711 -1173.44702148  -739.2869873\n",
            "   -905.27160645     0.          -598.47229004 -1172.59863281\n",
            "   -626.42889404  -787.99108887]\n",
            " [ -915.56982422     0.          -494.73043823  -576.60534668\n",
            "   -545.92358398  -622.53491211  -576.97796631  -554.21160889\n",
            "   -437.68200684  -566.02520752]\n",
            " [ -185.43850708  -728.24920654  -712.43902588  -330.38891602\n",
            "   -718.38653564     0.          -429.54486084  -738.22418213\n",
            "   -360.33566284  -592.76678467]\n",
            " [ -436.11367798  -192.71792603  -301.15667725  -241.05125427\n",
            "   -529.07629395  -303.07086182  -479.47436523  -481.77099609\n",
            "      0.          -466.4090271 ]\n",
            " [ -836.33532715     0.          -473.77111816  -577.88891602\n",
            "   -479.65750122  -537.41943359  -453.87103271  -569.94946289\n",
            "   -356.44570923  -526.16748047]\n",
            " [-1132.62109375 -1003.23083496  -638.57159424  -500.72943115\n",
            "   -835.89727783 -1138.93212891 -1437.58483887     0.\n",
            "   -858.48236084  -331.36593628]\n",
            " [ -353.96673584  -371.55065918  -278.94348145     0.\n",
            "   -271.22583008  -190.42578125  -470.88223267  -364.34661865\n",
            "    -29.19952393  -155.94308472]\n",
            " [ -736.1171875   -684.69573975  -691.72705078  -421.31610107\n",
            "    -21.59103394  -388.39440918  -567.84899902  -287.46716309\n",
            "   -280.29608154     0.        ]\n",
            " [ -767.91082764 -1085.06713867  -758.07427979  -704.83154297\n",
            "   -274.38476562  -659.93182373  -849.09033203   -79.30194092\n",
            "   -538.58612061     0.        ]\n",
            " [ -692.66448975  -579.64050293   -54.34127808  -240.32598877\n",
            "   -661.07800293  -716.40734863  -930.9942627      0.\n",
            "   -223.34925842  -213.10942078]\n",
            " [    0.          -900.69909668  -491.07901001  -504.19381714\n",
            "   -640.38690186  -380.37896729  -472.23461914  -504.32019043\n",
            "   -551.57501221  -512.54614258]\n",
            " [ -844.54187012  -724.83776855  -663.98309326  -499.43713379\n",
            "   -263.78051758  -492.02932739  -757.81152344  -121.17184448\n",
            "   -288.30532837     0.        ]\n",
            " [    0.         -1313.04553223  -806.13586426  -784.56787109\n",
            "  -1050.26379395  -597.47839355  -646.66687012  -914.71704102\n",
            "   -752.55773926  -952.74560547]\n",
            " [-1010.47949219     0.          -576.91766357  -494.23504639\n",
            "   -641.24310303  -758.02380371  -934.38574219  -427.36798096\n",
            "   -556.94049072  -621.69213867]]\n",
            "\n",
            "Labeled predictions: \n",
            "[(0, -487.1341552734375), (1, -206.1102294921875), (2, -421.7801513671875), (3, -318.37774658203125), (4, -303.4830017089844), (5, -195.57843017578125), (6, -397.751708984375), (7, -310.1751403808594), (8, 0.0), (9, -206.61590576171875)]\n",
            "\n",
            "Most likely answer: (8, 0.0)\n"
          ]
        }
      ],
      "source": [
        "response = predictor.predict(np.expand_dims(data, axis=1))\n",
        "print(\"Raw prediction result:\")\n",
        "print(response)\n",
        "print()\n",
        "\n",
        "labeled_predictions = list(zip(range(10), response[0]))\n",
        "print(\"Labeled predictions: \")\n",
        "print(labeled_predictions)\n",
        "print()\n",
        "\n",
        "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
        "print(\"Most likely answer: {}\".format(labeled_predictions[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E6djnPl2Nb0"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMAiczyC2Nb0"
      },
      "outputs": [],
      "source": [
        "sagemaker_session.delete_endpoint(endpoint_name=predictor.endpoint_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('torchtext_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
    "vscode": {
      "interpreter": {
        "hash": "23f8c5db1711cd8e1b53cf86a360c6e6888c4b0339673576cbef61b2c9b6977e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
